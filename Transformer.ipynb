{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPJAnMvTV51dRk+uAO8ONs9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kato1329/CATech/blob/main/Transformer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "5CCvstuVSDEJ"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "from torch import nn\n",
        "\n",
        "\n",
        "class ScaleDotProductAttention(nn.Module):\n",
        "  def __init__(self,d_k: int) -> None:\n",
        "    super().__init__()\n",
        "    self.d_k = d_k\n",
        "  def forward(\n",
        "      self,\n",
        "      q:torch.Tensor,\n",
        "      k:torch.Tensor,\n",
        "      v:torch.Tensor,\n",
        "      mask:torch.Tensor = None,\n",
        "  ) -> torch.Tensor:\n",
        "       ###Attentionを計算した後に値を正規化するために√dで割る。\n",
        "       scalar = np.sqrt(self.d_k)\n",
        "       ###Attentionの重みの宣言\n",
        "       ###torch.transpose(input,dim0,dim1)→Tensor\n",
        "       \"\"\"\n",
        "       transposeは入力した行列のdim0をdim1と転置する。\n",
        "       例えば\n",
        "       tensor = torch.Tensor([\n",
        "\n",
        "        [[1 ,2 ,3 ],\n",
        "        [4 ,5 ,6 ]],\n",
        "\n",
        "        [[7 ,8 ,9 ],\n",
        "        [10,11,12]],\n",
        "\n",
        "        [[13,14,15],\n",
        "        [16,17,18]]\n",
        "\n",
        "        ])\n",
        "        のサイズは(3,2,3)\n",
        "        この時torch.transpose(tensor,1,2)を行うと\n",
        "        2と3が入れ替わり、サイズは(3,3,2)となる。\n",
        "        従って得られるテンソルは\n",
        "        tensor([\n",
        "          [[ 1.,  4.],\n",
        "          [ 2.,  5.],\n",
        "          [ 3.,  6.]],\n",
        "\n",
        "          [[ 7., 10.],\n",
        "          [ 8., 11.],\n",
        "          [ 9., 12.]],\n",
        "\n",
        "          [[13., 16.],\n",
        "          [14., 17.],\n",
        "          [15., 18.]]\n",
        "          ])\n",
        "          となる。\n",
        "       \"\"\"\n",
        "       ###attentionの計算\n",
        "       attention_weight = torch.matmul(q,torch.transpose(k,1,2))/scalar\n",
        "       ###ここでattention_weightの次元はqの1次元目×kの1次元目となる。\n",
        "       if mask is not None:\n",
        "        if mask.dim() != attention_weight.dim():\n",
        "          raise ValueError(\n",
        "              \"mask.dim != attention_weight_dim,mask.dim = {},attention_dim = {}\".format(\n",
        "                  mask.dim(),attention_weight.dim()\n",
        "              )\n",
        "          )\n",
        "          \"\"\"\n",
        "          masked_fill_関数はboolで構成されたmask行列を受け取り、第一引数とする。受け取った\n",
        "          mask行列は隠したいところにtrueがおいてある行列であり、attention_weightと同じ形を\n",
        "          している。\n",
        "          第二引数にはmask行列のTrueがあるところに当てる数字が入る。torch.finfo.maxは引数の\n",
        "          型で表現することのできる最大の値を返し、それをマイナスにすることで-無限大を表現している\n",
        "          \"\"\"\n",
        "          attention_weight = attention_weight.data.masked_fill_(\n",
        "              mask,-torch.finfo(torch.float).max\n",
        "          )\n",
        "       attention_weight = nn.functional.softmax(attention_weight,dim=2)\n",
        "       ###出力の次元はbatch_size×qの1次元目×vの2次元目となる。\n",
        "       return torch.matmul(attention_weight,v)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "k = torch.Tensor([[\n",
        "    [1,-2,3,-4],\n",
        "    [-8,7,6,-5],\n",
        "    [10,9,12,11]\n",
        "]])\n",
        "k.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HDQtDhiGQJTf",
        "outputId": "3fc43ed5-16fd-40e5-c5fb-f764e9116839"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 3, 4])"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "q = torch.Tensor([\n",
        "    [1,-2,3,-4],\n",
        "    [-8,7,6,-5],\n",
        "    [10,9,12,11],\n",
        "    [1,-2,3,-4],\n",
        "    [-8,7,6,-5],\n",
        "    [10,9,12,11]\n",
        "])\n",
        "###size(1,4)\n",
        "k = torch.Tensor([[\n",
        "    [1,-2,3,-4],\n",
        "    [-8,7,6,-5],\n",
        "    [10,9,12,11]\n",
        "]\n",
        "])\n",
        "###size(1,3,4)\n",
        "v = k\n",
        "sample_scaler = ScaleDotProductAttention(4)\n",
        "print(sample_scaler.forward(q,k,v))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zGVNY6rp56r7",
        "outputId": "ddb623d5-345e-498b-ed5d-c0bc6be57eac"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[ 0.9918, -1.9918,  3.0027, -4.0009],\n",
            "         [-8.0000,  7.0000,  6.0000, -5.0000],\n",
            "         [10.0000,  9.0000, 12.0000, 11.0000],\n",
            "         [ 0.9918, -1.9918,  3.0027, -4.0009],\n",
            "         [-8.0000,  7.0000,  6.0000, -5.0000],\n",
            "         [10.0000,  9.0000, 12.0000, 11.0000]]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "  def __init__(self,d_model:int,h:int) -> None:\n",
        "    super().__init__()\n",
        "    self.d_model = d_model\n",
        "    self.h = h\n",
        "    self.d_k = d_model//h\n",
        "    self.d_v = d_model//h\n",
        "\n",
        "    self.W_k = nn.Parameter(\n",
        "        ###h個のd_model×d_k行列を作り、パラメータとする。\n",
        "        torch.Tensor(h,d_model,self.d_k)\n",
        "    )\n",
        "\n",
        "    self.W_q = nn.Parameter(\n",
        "        torch.Tensor(h,d_model,self.d_k)\n",
        "    )\n",
        "\n",
        "    self.W_v = nn.Parameter(\n",
        "        torch.Tensor(h,d_model,self.d_v)\n",
        "    )\n",
        "\n",
        "    self.scaled_dot_product_attention = ScaleDotProductAttention(self.d_k)\n",
        "\n",
        "    self.linear = nn.Linear(h*self.d_v,d_model)\n",
        "  def forward(\n",
        "      self,\n",
        "      q:torch.Tensor,\n",
        "      k:torch.Tensor,\n",
        "      v:torch.Tensor,\n",
        "      mask_3d:torch.Tensor = None,\n",
        "  ) -> torch.Tensor:\n",
        "     ###q.size(0)はTensorの行を表している。行→系列データの数よりbatch_sizeと一致する。\n",
        "     ###q.size(1)はTensorの列を表している。列→系列データの大きさよりseq_lenと一致する。\n",
        "     ###q.size(2)は系列一つのベクトルの長さを表している。従って単語の分散表現ベクトルに一致する。\n",
        "     ###つまり元のqのサイズは[batch_size,seq_len,k(=d_model)]である。\n",
        "     batch_size,seq_len = q.size(0),q.size(1)\n",
        "     \"\"\"\n",
        "     tensor.repeatは指定された次元に沿って行列を複製する。今回の場合予め作られたqの行列\n",
        "     の形を変えずにheadの数だけ複製し、新しい次元とする。\n",
        "     k,vも同様である。\n",
        "     \"\"\"\n",
        "     q = q.repeat(self.h,1,1,1)\n",
        "     k = k.repeat(self.h,1,1,1)\n",
        "     v = v.repeat(self.h,1,1,1)\n",
        "     ##i×j行列に長さkのベクトルが押し込まれている。×head数。\n",
        "     ##k×l行列のパラメータがある×head数\n",
        "     \"\"\"\n",
        "     二つの行列を式に従って掛け算するとj,k行列とk,l行列の掛け算が行われ、hijl行列となる。\n",
        "     従って掛け算によってk次元のベクトルの要素それぞれにパラメータの列の要素がかけられ、\n",
        "     足されていく。\n",
        "     下の操作によってQ,K,Vにはそれぞれ行列がかけられることになり、線形層の表現を獲得したといえる。\n",
        "     \"\"\"\n",
        "     q = torch.einsum(\n",
        "        ###k→d_model,l→d_kである。\n",
        "        \"hijk,hkl->hijl\",(q,self.W_q)\n",
        "     )\n",
        "     k = torch.einsum(\n",
        "        \"hijk,hkl->hijl\",(k,self.W_k)\n",
        "     )\n",
        "     v = torch.einsum(\n",
        "        \"hijk,hkl->hijl\",(v,self.W_v)\n",
        "     )\n",
        "     \"\"\"\n",
        "     qのサイズは[h,batch_size,seq_len,d_k]であり、\n",
        "     従って下の操作で次元を調整しても問題は生じない\n",
        "     \"\"\"\n",
        "     q = q.view(self.h*batch_size,seq_len,self.d_k)\n",
        "     k = k.view(self.h*batch_size,seq_len,self.d_k)\n",
        "     v = v.view(self.h*batch_size,seq_len,self.d_v)\n",
        "\n",
        "     if mask_3d is not None:\n",
        "      mask_3d = mask_3d.repeat(self.h,1,1)\n",
        "\n",
        "     ###内積注意の計算\n",
        "     attention_output = self.scaled_dot_product_attention(\n",
        "        q,k,v,mask_3d\n",
        "     )\n",
        "     ###線形層を通してd_model次元の出力を得る。\n",
        "     output = self.linear(attention_output)\n",
        "     ###outputはd_model次元のTensorとして出力される。\n",
        "     return output"
      ],
      "metadata": {
        "id": "C3Lp_oIAmQCu"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "\n",
        "class AddPositionalEncoding(nn.Module):\n",
        "    def __init__(\n",
        "        self, d_model: int, max_len: int, device: torch.device = torch.device(\"cpu\")\n",
        "    ) -> None:\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.max_len = max_len\n",
        "        positional_encoding_weight: torch.Tensor = self._initialize_weight().to(device)\n",
        "        self.register_buffer(\"positional_encoding_weight\", positional_encoding_weight)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        seq_len = x.size(1)\n",
        "        return x + self.positional_encoding_weight[:seq_len, :].unsqueeze(0)\n",
        "\n",
        "    def _get_positional_encoding(self, pos: int, i: int) -> float:\n",
        "        w = pos / (10000 ** (((2 * i) // 2) / self.d_model))\n",
        "        if i % 2 == 0:\n",
        "            return np.sin(w)\n",
        "        else:\n",
        "            return np.cos(w)\n",
        "\n",
        "    def _initialize_weight(self) -> torch.Tensor:\n",
        "        positional_encoding_weight = [\n",
        "            [self._get_positional_encoding(pos, i) for i in range(1, self.d_model + 1)]\n",
        "            for pos in range(1, self.max_len + 1)\n",
        "        ]\n",
        "        return torch.tensor(positional_encoding_weight).float()\n"
      ],
      "metadata": {
        "id": "wJKYkKBW2sTo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FFN(nn.Module):\n",
        "  def __init__(self,d_model:int,d_ff:int) -> None:\n",
        "    super().__init__()\n",
        "    self.linear1 = nn.Linear(d_model,d_ff)\n",
        "    self.linear2 = nn.Linear(d_ff,d_model)\n",
        "\n",
        "  def forward(self,x:torch,Tensor) -> torch.Tensor:\n",
        "    return self.linear2(nn.functional.relu(self.linear1(x)))"
      ],
      "metadata": {
        "id": "-Vr3sYYh7K8r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "class TransformerEncoderLayer(nn.Module):\n",
        "  def __init__(\n",
        "      self,\n",
        "      d_model:int,###multiheadattentionの次元数\n",
        "      d_ff:int,###FFNの出力の次元数\n",
        "      heads_num:int,###multiheadattentionのhead数\n",
        "      dropout_rate:float,###dropout層のdropout確率\n",
        "      layer_norm_eps:float,###LayerNorm層におけるepsの値。デフォルトは1e-05\n",
        "  ) -> None:\n",
        "    super().__init__()\n",
        "\n",
        "    self.multi_head_attention = MultiHeadAttention(d_model,heads_num)\n",
        "    self.dropout_self_attention = nn.Dropout(dropout_rate)\n",
        "    self.layer_norm_self_attention = nn.LayerNorm(d_model,eps=layer_norm_eps)\n",
        "\n",
        "    self.ffn = FFN(d_model,d_ff)\n",
        "    self.dropout_ffn = nn.Dropout(dropout_rate)\n",
        "    self.layer_norm_ffn = nn.LayerNorm(d_model,eps=layer_norm_eps)\n",
        "\n",
        "  def forward(self,x:torch.Tensor,mask:torch.Tensor = None) -> torch.Tensor:\n",
        "      x = self.layer_norm_self_attention(self.__self_attention_block(x,mask)+x)\n",
        "      x = self.layer_norm_ffn(self.__feed_forward_block(x)+x)\n",
        "      return x\n",
        "\n",
        "  def __self_attention_block(self,x:torch.Tensor,mask:torch.Tensor) -> torch.Tensor:\n",
        "    x = self.multi_head_attention(x,x,x,mask)\n",
        "    return self.dropout_self_attention(x)\n",
        "\n",
        "  def __feed_forward_block(self,x:torch.Tensor) -> torch.Tensor:\n",
        "    return self.dropout_ffn(self.ffn(x))\n",
        "\n",
        "class TransformerEncoder(nn.Module):\n",
        "  def __init__(\n",
        "      self,\n",
        "      vocab_size:int,\n",
        "      max_len:int,\n",
        "      pad_idx:int,\n",
        "      d_model:int,\n",
        "      N:int,\n",
        "      d_ff:int,\n",
        "      heads_num:int,\n",
        "      dropout_rate:float,\n",
        "      layer_norm_eps:float,\n",
        "      device:torch.device=torch.device(\"cpu\")\n",
        "  ) -> None:\n",
        "    super().__init__()\n",
        "    self.embedding = nn.Embedding(vocab_size,d_model,pad_idx)\n",
        "    self.positional_encoding = AddPositionalEncoding(d_model,max_len,device)\n",
        "    self.encoder_layers = nn.ModuleList(\n",
        "        [\n",
        "            TransformerEncoderLayer(\n",
        "                d_model,d_ff,heads_num,dropout_rate,layer_norm_eps\n",
        "            )\n",
        "            for _ in range(N)\n",
        "        ]\n",
        "    )\n",
        "\n",
        "  def forward(self,x:torch.Tensor,mask:torch.Tensor = None) -> torch.Tensor:\n",
        "      x = self.embedding(x)\n",
        "      x = self.positional_encoding(x)\n",
        "      for encoder_layer in self.encoder_layers:\n",
        "        x = encoder_layer(x,mask)\n",
        "      return x\n"
      ],
      "metadata": {
        "id": "u7mJC6x374T9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerDecoderLayer(nn.Module):\n",
        "  def __init__(\n",
        "      self,\n",
        "      d_model:int,\n",
        "      d_ff:int,\n",
        "      heads_num:int,\n",
        "      dropout_rate:float,\n",
        "      layer_norm_eps:float,\n",
        "  ):\n",
        "    super().__init__()\n",
        "    self.self_attention = MultiHeadAttention(d_model,heads_num)\n",
        "    self.dropout_self_attention = nn.Dropout(dropout_rate)\n",
        "    self.layer_norm_self_attention = LayerNorm(d_model,eps=layer_norm_eps)\n",
        "\n",
        "    self.src_tgt_attention = MultiHeadAttention(d_model,heads_num)\n",
        "    self.dropout_src_tgt_attention = nn.Dropout(dropout_rate)\n",
        "    self.layer_norm_src_tgt_attention = LayerNorm(d_model,eps=layer_norm_eps)\n",
        "\n",
        "    self.ffn = FFN(d_model,d_ff)\n",
        "    self.dropout_ffn = nn.Dropout(dropout_rate)\n",
        "    self.layer_norm_ffn = LayerNorm(d_model,eps=layer_norm_eps)\n",
        "\n",
        "  def forward(\n",
        "      self,\n",
        "      tgt:torch.Tensor,\n",
        "      src:torch.Tensor,\n",
        "      mask_src_tgt:torch.Tensor,\n",
        "      mask_self:torch.Tensor,\n",
        "  ) -> torch.Tensor:\n",
        "    tgt = self.layer_norm_self_attention(\n",
        "        tgt + self.__self_attention_block(tgt,mask_self)\n",
        "    )\n",
        "\n",
        "    x = self.layer_norm_src_tgt_attention(\n",
        "        tgt + self.__src_tgt_attention_block(src,tgt,mask_src_tgt)\n",
        "    )\n",
        "\n",
        "    x = self.layer_norm_ffn(x+self.__feed_forward_block(x))\n",
        "\n",
        "    return x\n",
        "  def __src_tgt_attention_block(\n",
        "      self,src:torch.Tensor,tgt:torch.Tensor,mask:torch.Tensor\n",
        "  ) -> torch.Tensor:\n",
        "    return self.dropout_src_tgt_attention(\n",
        "        self.src_tgt_attention(tgt,src,src,mask)\n",
        "    )\n",
        "  def __self_attention_block(\n",
        "      self,x:torch.Tensor,mask:torch.Tensor\n",
        "  ):\n",
        "    return self.dropout_self_attention(self.self_attention(x,x,x,mask))\n",
        "  def __feed_forward_block(self,x:torch.Tensor) -> torch.Tensor:\n",
        "    return self.dropout_ffn(self.ffn(x))\n"
      ],
      "metadata": {
        "id": "vyP2zoUT74eX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerDecoder(nn.Module):\n",
        "  def __init__(\n",
        "      self,\n",
        "      tgt_vocab_size:int,\n",
        "      max_len:int,\n",
        "      pad_idx:int,\n",
        "      d_model:int,\n",
        "      N:int,\n",
        "      d_ff:int,\n",
        "      heads_num:int,\n",
        "      dropout_rate:float,\n",
        "      layer_norm_eps:float,\n",
        "      device:torch.device = torch.device('cpu')\n",
        "  ) -> None:\n",
        "    super().__init__()\n",
        "    self.embedding = Embedding(tgt_vocab_size,d_model,pad_idx)\n",
        "    self.positional_encoding = AddPositionalEncoding(d_model,max_len,device)\n",
        "    self.decoder_layers = nn.ModuleList(\n",
        "        [\n",
        "            TransformerDecoderLayer(\n",
        "                d_model,d_ff,heads_num,dropout_rate,layer_norm_eps\n",
        "            )\n",
        "            for _ in range(N)\n",
        "        ]\n",
        "    )\n",
        "  def forward(\n",
        "      self,\n",
        "      tgt:torch.Tensor,\n",
        "      src:torch.Tensor,\n",
        "      mask_src_tgt:torch.Tensor,\n",
        "      mask_self:torch.Tensor\n",
        "  ) -> torch.Tensor:\n",
        "    tgt = self.embedding(tgt)\n",
        "    tgt = self.positional_encoding(tgt)\n",
        "    for decoder_layer in self.decoder_layers:\n",
        "      tgt = decoder_layer(\n",
        "          tgt,\n",
        "          src,\n",
        "          mask_src_tgt,\n",
        "          mask_self\n",
        "      )\n",
        "    return tgt"
      ],
      "metadata": {
        "id": "8aQ5fbeTQQkk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Transformer(nn.Module):\n",
        "  def __init__(\n",
        "      self,\n",
        "      src_vocab_size:int,\n",
        "      tgt_vocab_size:int,\n",
        "      max_len:int,\n",
        "      d_model:int=512,\n",
        "      heads_num:int=8,\n",
        "      d_ff:int=2048,\n",
        "      N:int=6,\n",
        "      dropout_rate:float=0.1,\n",
        "      layer_norm_eps:float=1e-5,\n",
        "      pad_idx:int=0,\n",
        "      device:torch.device = torch.device('cpu')\n",
        "  ):\n",
        "    super().__init__()\n",
        "    self.src_vocab_size = src_vocab_size\n",
        "    self.tgt_vocab_size = tgt_vocab_size\n",
        "    self.d_model = d_model\n",
        "    self.max_len = max_len\n",
        "    self.heads_num = heads_num\n",
        "    self.d_ff = d_ff\n",
        "    self.N = N\n",
        "    self.dropout_rate = dropout_rate\n",
        "    self.layer_norm_eps = layer_norm_eps\n",
        "    self.pad_idx = pad_idx\n",
        "    self.device = device\n",
        "\n",
        "    self.encoder = TransformerEncoder(\n",
        "        src_vocab_size,\n",
        "        max_len,\n",
        "        pad_idx,\n",
        "        d_model,\n",
        "        N,\n",
        "        d_ff,\n",
        "        heads_num,\n",
        "        dropout_rate,\n",
        "        layer_norm_eps,\n",
        "        device\n",
        "    )\n",
        "\n",
        "    self.decoder = TransformerDecoder(\n",
        "        tgt_vocab_size,\n",
        "        max_len,\n",
        "        pad_idx,\n",
        "        d_model,\n",
        "        N,\n",
        "        d_ff,\n",
        "        heads_num,\n",
        "        dropout_rate,\n",
        "        layer_norm_eps,\n",
        "        device\n",
        "    )\n",
        "\n",
        "    self.linear = nn.Linear(d_model,tgt_vocab_size)\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        src:torch.Tensor,\n",
        "        tgt:torch.Tensor\n",
        "    ) -> torch.Tensor:\n",
        "      pad_mask_src = self._pad_mask(src)\n",
        "      src = self.encoder(src,pad_mask_src)\n",
        "\n",
        "      mask_self_attn = torch.logical_or(\n",
        "          self._subsequent_mask(tgt),self._pad_mask(tgt)\n",
        "      )\n",
        "      dec_output = self.decoder(tgt,src,pad_mask_src,mask_self_attn)\n",
        "\n",
        "      return self.linear(dec_output)\n",
        "\n",
        "    def _pad_mask(self,x:torch.Tensor) -> torch.Tensor:\n",
        "      seq_len = x.size(1)\n",
        "      ###これattention_maskつくるのめっちゃ楽じゃね\n",
        "      mask = x.eq(self.pad_idx)\n",
        "      mask = mask.unsqueeze(1)\n",
        "      mask = mask.repeat(1,seq_len,1)\n",
        "      return mask.to(self.device)\n",
        "\n",
        "    def _subsequent_mask(self,x:torch.Tensor) -> torch.Tensor:\n",
        "      batch_size = x.size(0)\n",
        "      max_len = x.size(1)\n",
        "      return (\n",
        "          torch.trill(torch.ones(batch_size,max_len,max_Len)).eq(0.to(self.device))\n",
        "      )"
      ],
      "metadata": {
        "id": "QUGXRD5w6PTF"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}